---
uid: NoSQLReportingIntroduction
author: fcerbell
title: Introduction to NoSQL reporting
layout: post
lang: en
#description:
#category: Test
#categories
#tags
#date
published: true
---

Whatever the database technology is, there is a business application using it
and there is most of the time a reporting need on the business data. I will try
to introduce how to design reporting data, the concepts are applicable to
relational SQL databases and to non-relational NOSQL databases either.
There are two distinct reporting needs : the operational reporting, which
applies to live operational data to create operational reports such as
invoices, order forms, assets inventories, and so on, and  the business
intelligence reporting which is higher level reporting to give trends and to
help people to take decisions.


* TOC
{:toc}

# Reporting general concepts

## Operational reporting

This reporting is directly used by the business application to deliver
operational reports and the live data. The business needs this kind of reports.
As an example, if the business is to plan train travels, the business
application enables to find a train route for the user with appropriate date
and time, then to book the train and to charge the user's bank account. At the
end, the application needs to emit a ticket, which is an operational report. If
the business is to send parcels to customers, then a label has to be printed
depending on the size, weight, destination, preferred delivery service,
delivery level, sometimes with a specific barcode that need to be readable by
the delivery service optical readers.

In this context, the reporting is fully integrated into the business
application and is part of it. The application data model was designed for the
processing and the delivering of the train tickets. The report processing can
be either developped in the application, or a third party library embedded into
the application, or it can be a standalone third party reporting application.
Some third party tools can be embedded, other cannot, some are specific (stream
oriented or query oriented).

### Source data passed

Whatever the archiecture is, the reporting code needs to have the source data
to create the report. In this first case, the source data is queried or
generated by the application and passed to the reporting code (legacy, embedded
or externalized). This case is quite simple as the business application already
knows the business concepts, the business objects, the data model used in
the database. It also knows the needed source data to generate the report and
knows how to generate it (from queries or from computation).

So, the application eventually queries the data storage, using the usual API,
and pass to the reporting function the source data, the report template and the
output format to generate the report.

### Source data queried

In this second use case, the business application passes very few information
to the reporting function (internal, embedded or external), it will be up to
this reporting function to use the information to build the report, which means
that the reporting function needs to be able to compute the source data from
the information or to generate and execute a query on the data storage to get
its source data. This logic is usually included in the report template, it is
not supposed to be a business logic, but more a data access logic. An invoice
template does not need to know what is an invoice, but it has to know where to
find the invoice number, the customer's name and address, the item lines with
their prices, the VAT, and to know where to place them in the output report.

This use case is quite different as the reporting function needs to be able to 
access the data in the data storage. It needs to know how to create a query,
how to execute it, how to read the resultset. The stream based reporting tools
are not really concerned here, but all the query based one are.

Some reporting tools can be extended to learn how to generate queries, how to
execute them and how to understand their resultsets. Most of the time, they are
opensource reporting software such as Tibco's JasperReports library or Pentaho.
But a lot of reporting tools can not be extended, they are provided with a set
of connectors, period. The only flexibility for these tools is provided with
some generic connectors, most of the time an ODBC connector, a JDBC connector,
a CSV file connector, an XLS file connector and an XML file connector,
sometimes there are also an XML webservice connector, and an ODBO connector.

Basically, it means that the report can use an SQL query, an XPath query, a
flat (no joins) filter on CSV, a legacy query on XLS, or an MDX query on an
OLAP datasource, but nothing else. When the source data are stored in any other
storage (key-value, JSON, graph, wide-columns, ...), there will be an issue.
Hopefully, some NOSQL databases provide a SQL interface, such as N1QL for
Couchbase.

## Business intelligence reporting

In the business intelligence reporting, the business application does not
provide the source data and ask the function to generate a report. Most of the
time, the application provides a source data connection and let the user create
his own analysis. It means that the reporting function needs to be able to
connect to the data source to execute a predefined query, or to generate an
AdHoc query to return the exact resultset asked by the user, leveraging the
underlying data storage technology by pushing down the aggregations and
filters.

### Live, on-line operational data

In business intelligence reporting, there is a commonly accepted architecture.
The operational live data are stored in a database, historically a relational
SQL database and the data are normalized. These are live data, online data.

### The Operational Data Store (ODS)

Then, they are pushed to an ODS (Operational Data Store) which is usually a
relational SQL database, too, with a normalized schema. It can store some chosen
historical data. The idea is to feed it enough to be able to feed the next level
and to empty it. As an example, the ODS can be fed daily, then it is used to
feed the data warehouse (DWH) and it is cleaned to begin a new month.

### The Datawarehouse (DWH)

The next level is the data warehouse (DWH), it is not supposed to have a
normalized schema, but a schema which fits to the reporting needs. Most of the
time, it is stored in a relational SQL database, with a star schema or a
snowflake schema, which are highly denormalized. The DWH is supposed to store
clean data, preaggregated data (no useless data, just in case of...), quality
data. If the reporting smaller granularity is the day, you should not find
hourly data in the DWH. There are usually two kinds of tables : facts tables to
store the actual indicators values, and the dimensions (or reference) tables to
store the possible analyzis axis.

#### Dimensions 

Dimensions are the different axis to analyze the key performance indicators
(KPI). Common dimensions are a time dimension, and a geographic dimensions, but
there are a lot of other dimensions implemented in the DWH, depending on the
business (sales territory, sales market, customer segmentation, product
category, product line, economic regions, ...). We will focus on the geographic
and time dimensions as they are typical dimensions.

A dimension is made of hierarchies. Why hierarchies and not hierarchy ? Because
if there was only one hierarchy, there would be no dimension need ! Dimensions
are a concept. 

##### Hierarchies

The time dimension is the concept of time, nothing else. It does
not describe how the time is represented. Business may need to analyze the KPI
on monthes, on weeks, on seasons, on fiscal years, ... Each of them are
incompatible with the others, each one will be a different time dimension
implementation, a different hierarchy. The geographic dimension would also have
several hierarchies inside : Economic areas, countries, sales territories, ...

###### Levels

Each hierarchy is made of levels, here are some level examples for the time
dimension's hierarchies :

* Year, Half, Quarter, Month, Date
* Year, Week, Day of the week
* Year range, Season
* Fiscal year, Half, Quarter, Month (remember to not store too smaller
  granularity than needed)

In the time dimension, the date level can store extra information such as
week-end or not, holidays or not, first/last business day of the week/month or
not. The year level could also store information such as leap year or not. From
a business point of view, these extra information can be used as facets or
filters.

* Year : isLeap
* Day : isWeekDay, isHoliday, isFirstDayOfMonth, isLastDayOfMonth, ...

As the DWH is usually stored in a relational SQL database, it has a
table/relation schema. For sure, a hierarchy can be normalized with a table for
the years, a table for the halfs, and another for each levels, with a
parent-child relationship. This leads to a *snowflake* schema at the end, but as
I said previously, the DWH is not normalized, so the hierarchies can be
flattened to have only one table for each hierarchy, with one record for each
smaller granularity (the day) grouping alltogether the year, half, quarter,
month and day information. This makes the records bigger, but minimize the hops
(joins) and provide good performances with relevant indexes (at the price of
even more disk space needed).

#### Facts tables

The fact tables are simpler to understand. There is one fact table to store all
preaggregated KPIs which share the same hierarchies. Each record is the KPIs
aggregation at the cross of the hierarchies. Given our example, if some KPIs are
sharing the Year/Month/Date and the Continent/Country/City hierarchy, there
would be a record for each Date/City combination. That's why useless levels and
granularities should be avoided, it leads to disk space useage and to extra
computation when asking for useful granularity aggregations, ie storing hourly
data leade to 24*NbCities more records and there will always be a computation
for the daily aggregation which is the lowest level asked by the business,
instead of saving space and having immediate static results.


# Reporting with Couchbase

## Operational reporting

## Couchbase as an ODS

## Couchbase as a DWH

## Couchbase as a DMT with views


---

Concernant la possibilité de connecter des outils décisionnels à Couchbase. Il faut commencer par prendre en compte que Couchbase est utilisée comme base de données opérationnelle. Une architecture décisionnelle typique comporte théoriquement une base de type ODS (Operational Data Store) avec un schéma opérationnel, un Datawarehouse (DWH) ayant un schéma démoralisé plutôt en étoile ou en flocon et comportant des données nettoyées, validée et potentiellement pré-agrégées, et des Datamarts (DMT) avec des données agrégées dans un schéma en étoile ou flocon (pour faire du ROLAP) ou un stockage spécialisé (pour du HOLAP ou du MOLAP), le tout lié par des traitements Extract-Transform-Load (ETL) ou Extract-Load-Transform (ELT) et de validation/redressement des données. Il est cependant toujours possible de connecter un outil d’analyse décisionnelle à une base opérationnelle en ayant conscience des impacts en terme de dimensionnement de l’infrastructure et de qualité des données.

Dans ce contexte, la plupart des outils décisionnels (Qlikview, Tableau, BO, Pentaho, Jaspersoft) ne savent pas toujours dialoguer avec une base de données non-relationnelle et NoSQL. Ils utilisent principalement le SQL ou le MDX pour interroger les données. Certains outils (comme Pentaho et Jaspersoft) peuvent être étendus pour utiliser d’autres langages de requête. Un accès purement clé/valeur n’est pas toujours suffisant pour créer des rapports décisionnels sur des documents opérationnels non-modélisés dans un but décisionnel. Il est donc possible d’étendre ces outils pour leur permettre d’utiliser un accès clé/valeur, à condition d’avoir par ailleurs des traitements construisant des documents agrégés. Il est également possible d’étendre ces mêmes outils pour leur permettre d’exploiter les vues (MapReduce) de Couchbase et ainsi de bénéficier d’agrégations partiellement précalculées et distribuées, particulièrement les vues multi-dimensionnelles qui peuvent s’apparenter à un stockage OLAP. Mais surtout, Couchbase a l’avantage d’être une base de données NOSQL (dans le sens Not Only SQL) et de pouvoir interpréter des requêtes SQL grâce à ses pilotes ODBC et JDBC. Cela signifie qu’il est possible de connecter tout outil décisionnel en mesure d’utiliser une connection ODBC ou JDBC en lui fournissant une requête comme source de données. Dès lors, il devient possible d’exprimer des requêtes en SQL sur les documents opérationnels, et de les utiliser directement comme sources de données depuis les outils décisionnels.

$$
\begin{align*}
  & \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  & (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) & \cdots & \phi(e_1, e_n) \\
      \vdots & \ddots & \vdots \\
      \phi(e_n, e_1) & \cdots & \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{align*}
$$

$$ 5 + 5 $$


* H1 : # Header 1
* H2 : ## Header 2
* H3 : ### Header 3
* H4 : #### Header 4
* H5 : ##### Header 5
* H6 : ###### Header 6
* Links : [Label](URL 'title')
* Links : [Label][linkid]
* Bold : **Bold**
* Italicize : *Italics*
* Strike-through : ~~text~~
* Highlight : ==text==
* Paragraphs : Line space between paragraphs
* Line break : Add two spaces to the end of the line
* Lists : * an asterisk for every new list item.
* Quotes : > Quote
* Inline Code : `alert('Hello World');`
* Horizontal Rule (HR) : --------
* Footnote[^1]

[linkid]: http://www.example.com/ "Optional Title"

[^1]: This is my first footnote



